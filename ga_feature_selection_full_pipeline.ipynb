{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from joblib import Parallel, delayed\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFIER = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Start Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_frac=0.01\n",
    "\n",
    "# Load dataset as pandas DataFrame\n",
    "df_train = pd.read_parquet('./data/cic_iomt_2024_wifi_mqtt_train.parquet')\n",
    "df_test = pd.read_parquet('./data/cic_iomt_2024_wifi_mqtt_test.parquet')  \n",
    "\n",
    "# Create sample DataFrame for feature selection\n",
    "df_train_sample = df_train.sample(frac=my_frac, random_state=1984)      \n",
    "df_test_sample = df_test.sample(frac=my_frac, random_state=1984)   \n",
    "\n",
    "# Create sample X and y from train and test, convert to numpy arrays\n",
    "X_train_sample = df_train_sample.drop(columns=['label', 'class_label', 'category_label', 'attack_label']).to_numpy()\n",
    "y_train_sample_2 = df_train_sample['class_label'].to_numpy()\n",
    "y_train_sample_6 = df_train_sample['category_label'].to_numpy()\n",
    "y_train_sample_19 = df_train_sample['attack_label'].to_numpy()\n",
    "\n",
    "\n",
    "X_test_sample = df_test_sample.drop(columns=['label', 'class_label', 'category_label', 'attack_label']).to_numpy()\n",
    "y_test_sample_2 = df_test_sample['class_label'].to_numpy()\n",
    "y_test_sample_6 = df_test_sample['category_label'].to_numpy()\n",
    "y_test_sample_19 = df_test_sample['attack_label'].to_numpy()\n",
    "\n",
    "\n",
    "# Create full data X and y from train and test, convert to numpy arrays\n",
    "X_train_full = df_train_sample.drop(columns=['label', 'class_label', 'category_label', 'attack_label']).to_numpy()\n",
    "y_train_full_2 = df_train_sample['class_label'].to_numpy()\n",
    "y_train_full_6 = df_train_sample['category_label'].to_numpy()\n",
    "y_train_full_19 = df_train_sample['attack_label'].to_numpy()\n",
    "\n",
    "\n",
    "X_test_full = df_test_sample.drop(columns=['label', 'class_label', 'category_label', 'attack_label']).to_numpy()\n",
    "y_test_full_2 = df_test_sample['class_label'].to_numpy()\n",
    "y_test_full_6 = df_test_sample['category_label'].to_numpy()\n",
    "y_test_full_19 = df_test_sample['attack_label'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use hyperparameters from the CICIoMT2024 to establish benchmarks for classification on the dataset. We will then use pso to select the best features and compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_benchmark = LogisticRegression(\n",
    "    penalty='l2', \n",
    "    dual=False, \n",
    "    tol=0.0001, \n",
    "    C=1.0, \n",
    "    fit_intercept=True, \n",
    "    intercept_scaling=1, \n",
    "    solver='lbfgs', \n",
    "    max_iter=100,\n",
    "    warm_start=False, \n",
    "    n_jobs=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_benchmark.fit(X_train_sample, y_train_sample_2)\n",
    "y_pred_lr_benchmark_2 = lr_benchmark.predict(X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the classification report\n",
    "report_lr_benchmark_2 = classification_report(y_test_sample_2, y_pred_lr_benchmark_2, output_dict=True)\n",
    "print(classification_report(y_test_sample_2, y_pred_lr_benchmark_2))\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy_lr_benchmark_2 = accuracy_score(y_test_sample_2, y_pred_lr_benchmark_2)\n",
    "\n",
    "# print accuracy with 5 decimal places\n",
    "print(f\"Accuracy: {accuracy_lr_benchmark_2:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_benchmark.fit(X_train_sample, y_train_sample_6)\n",
    "y_pred_lr_benchmark_6 = lr_benchmark.predict(X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the classification report\n",
    "report_lr_benchmark_6 = classification_report(y_test_sample_6, y_pred_lr_benchmark_6, output_dict=True)\n",
    "print(classification_report(y_test_sample_6, y_pred_lr_benchmark_6))\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy_lr_benchmark_6 = accuracy_score(y_test_sample_6, y_pred_lr_benchmark_6)\n",
    "\n",
    "# print accuracy with 5 decimal places\n",
    "print(f\"Accuracy: {accuracy_lr_benchmark_6:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19 Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_benchmark.fit(X_train_sample, y_train_sample_19)\n",
    "y_pred_lr_benchmark_19 = lr_benchmark.predict(X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the classification report\n",
    "report_lr_benchmark_19 = classification_report(y_test_sample_19, y_pred_lr_benchmark_19, output_dict=True)\n",
    "print(classification_report(y_test_sample_19, y_pred_lr_benchmark_19))\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy_lr_benchmark_19 = accuracy_score(y_test_sample_19, y_pred_lr_benchmark_19)\n",
    "\n",
    "# print accuracy with 5 decimal places\n",
    "print(f\"Accuracy: {accuracy_lr_benchmark_19:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_benchmark = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(), \n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0, \n",
    "    algorithm='SAMME.R', \n",
    "    random_state=1984\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_benchmark.fit(X_train_sample, y_train_sample_2)\n",
    "y_pred_ada_benchmark_2 = ada_benchmark.predict(X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the classification report\n",
    "report_ada_benchmark_2 = classification_report(y_test_sample_2, y_pred_ada_benchmark_2, output_dict=True)\n",
    "print(classification_report(y_test_sample_2, y_pred_ada_benchmark_2))\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy_ada_benchmark_2 = accuracy_score(y_test_sample_2, y_pred_ada_benchmark_2)\n",
    "\n",
    "# print accuracy with 5 decimal places\n",
    "print(f\"Accuracy: {accuracy_ada_benchmark_2:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_benchmark.fit(X_train_sample, y_train_sample_6)\n",
    "y_pred_ada_benchmark_6 = ada_benchmark.predict(X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the classification report\n",
    "report_ada_benchmark_6 = classification_report(y_test_sample_6, y_pred_ada_benchmark_6, output_dict=True)\n",
    "print(classification_report(y_test_sample_6, y_pred_ada_benchmark_6))\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy_ada_benchmark_6 = accuracy_score(y_test_sample_6, y_pred_ada_benchmark_6)\n",
    "\n",
    "# print accuracy with 5 decimal places\n",
    "print(f\"Accuracy: {accuracy_ada_benchmark_6:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19 Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_benchmark.fit(X_train_sample, y_train_sample_19)\n",
    "y_pred_ada_benchmark_19 = ada_benchmark.predict(X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_ada_benchmark_19 = classification_report(y_test_sample_19, y_pred_ada_benchmark_19, output_dict=True)\n",
    "print(classification_report(y_test_sample_19, y_pred_ada_benchmark_19))\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy_ada_benchmark_19 = accuracy_score(y_test_sample_19, y_pred_ada_benchmark_19)\n",
    "\n",
    "# print accuracy with 5 decimal places\n",
    "print(f\"Accuracy: {accuracy_ada_benchmark_19:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_benchmark = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    criterion='gini', \n",
    "    min_samples_split=2, \n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    max_features='sqrt', \n",
    "    min_impurity_decrease=0.0,\n",
    "    bootstrap=True, \n",
    "    oob_score=False, \n",
    "    warm_start=False, \n",
    "    ccp_alpha=0.0, \n",
    "    n_jobs=-1, \n",
    "    random_state=1984\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_benchmark.fit(X_train_sample, y_train_sample_2) \n",
    "y_pred_rf_benchmark_2 = rf_benchmark.predict(X_test_sample)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the classification report\n",
    "report_rf_benchmark_2 = classification_report(y_test_sample_2, y_pred_rf_benchmark_2, output_dict=True)\n",
    "print(classification_report(y_test_sample_2, y_pred_rf_benchmark_2))\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy_rf_benchmark_2 = accuracy_score(y_test_sample_2, y_pred_rf_benchmark_2)\n",
    "\n",
    "# print accuracy with 5 decimal places\n",
    "print(f\"Accuracy: {accuracy_rf_benchmark_2:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_benchmark.fit(X_train_sample, y_train_sample_6) \n",
    "y_pred_rf_benchmark_6 = rf_benchmark.predict(X_test_sample)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the classification report\n",
    "report_rf_benchmark_6 = classification_report(y_test_sample_6, y_pred_rf_benchmark_6, output_dict=True)\n",
    "print(classification_report(y_test_sample_6, y_pred_rf_benchmark_6))\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy_rf_benchmark_6 = accuracy_score(y_test_sample_6, y_pred_rf_benchmark_6)\n",
    "\n",
    "# print accuracy with 5 decimal places\n",
    "print(f\"Accuracy: {accuracy_rf_benchmark_6:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19 Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_benchmark.fit(X_train_sample, y_train_sample_19) \n",
    "y_pred_rf_benchmark_19 = rf_benchmark.predict(X_test_sample)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the classification report\n",
    "report_rf_benchmark_19 = classification_report(y_test_sample_19, y_pred_rf_benchmark_19, output_dict=True)\n",
    "print(classification_report(y_test_sample_19, y_pred_rf_benchmark_19))\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy_rf_benchmark_19 = accuracy_score(y_test_sample_19, y_pred_rf_benchmark_19)\n",
    "\n",
    "# print accuracy with 5 decimal places\n",
    "print(f\"Accuracy: {accuracy_rf_benchmark_19:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GA Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection With GA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below (up until \"FUNCTION: Feature reduction with Genetics Algorithm\") is created for interactive exploration of Genetics Algorithm. It should be removed from current notebook after users are comforatable with implementation. Function ga_feature_selection() is a copy of the code chunks below excluding printing statements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GA Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genetic Algorithm parameters\n",
    "population_size = 20            # number of individuals in the population. HAS TO BE AN EVEN NUMBER!!!!!\n",
    "n_generations = 50              # maximum number of generations\n",
    "mutation_rate = 0.1             # probability of mutation\n",
    "#fitness_threshold = 1          # fitness goal (threshold for stopping)\n",
    "#stagnation_limit = 3           # number of generations without improvement before stopping\n",
    "num_elites = 2                  # number of elite individuals to keep in each generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitness Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the fitness function for evaluating feature subsets\n",
    "def fitness_function(individual, y_train_sample, y_test_sample, CLASSIFIER=\"rf\", metric=\"accuracy\"):\n",
    "    # function selects the features based on the individual's genes - features with values > 0.5 are selected\n",
    "    selected_features = np.where(individual == 1)[0]  # select features based on individual genes\n",
    "    \n",
    "    if len(selected_features) == 0:                   # avoid empty feature set\n",
    "        return 0\n",
    "    \n",
    "    X_train_selected = X_train_sample[:, selected_features]\n",
    "    X_test_selected = X_test_sample[:, selected_features]\n",
    "\n",
    "    if CLASSIFIER == 'lr':\n",
    "        lr = LogisticRegression(**lr_benchmark.get_params())\n",
    "        lr.fit(X_train_selected, y_train_sample)\n",
    "        y_pred = lr.predict(X_test_selected)\n",
    "\n",
    "    elif CLASSIFIER == 'ada':\n",
    "        ada = AdaBoostClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=1984)\n",
    "        ada.fit(X_train_selected, y_train_sample)               \n",
    "        y_pred = ada.predict(X_test_selected) \n",
    "    \n",
    "    elif CLASSIFIER == 'rf':\n",
    "        rf = RandomForestClassifier(**rf_benchmark.get_params())\n",
    "        rf.fit(X_train_selected, y_train_sample)               \n",
    "        y_pred = rf.predict(X_test_selected) \n",
    "\n",
    "    # Calculate the chosen metric\n",
    "    if metric == 'accuracy':\n",
    "        score = accuracy_score(y_test_sample, y_pred)\n",
    "    elif metric == 'precision':\n",
    "        score = precision_score(y_test_sample, y_pred, average='weighted')\n",
    "    elif metric == 'recall':\n",
    "        score = recall_score(y_test_sample, y_pred, average='weighted')\n",
    "    elif metric == 'f1':\n",
    "        score = f1_score(y_test_sample, y_pred, average='weighted')\n",
    "    else:\n",
    "        raise ValueError(\"Invalid metric. Choose from 'accuracy', 'precision', 'recall', or 'f1'.\")\n",
    "\n",
    "    return score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Population and Objects for Collecting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of features for individual length\n",
    "n_features = X_train_sample.shape[1]   # number of features in dataset (45)\n",
    "\n",
    "# initialize population with random values between 0 and 1 (individuals represent feature subsets)\n",
    "# population = np.random.rand(population_size, n_features)\n",
    "population = np.random.randint(2, size=(population_size, n_features))\n",
    "\n",
    "# initialize variables to track the best fitness and stagnation count\n",
    "best_fitness_overall = 0\n",
    "best_individual_overall = None\n",
    "no_improvement_count = 0\n",
    "termination_reason = None  # To store the reason for termination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genetic Algorithm main loop for feature selection\n",
    "for generation in range(n_generations):\n",
    "    \n",
    "    # printing population at the start of each loop\n",
    "    print(f\"LOOP STEP: Generation {generation + 1}\")\n",
    "    print(f\"Population {generation + 1}: {population}\")\n",
    "    \n",
    "    # using fitness_function (defined above) to calculate fitness score (accuracy, f1 ...) for each individual in population\n",
    "    fitness_scores = np.array([fitness_function(individual, y_train_sample=y_train_sample_2, y_test_sample=y_test_sample_2) for individual in population])\n",
    "    \n",
    "    ### PROGRESS UPDATE: display calculated fitness scores\n",
    "    print(f\"Fitness Scores: {fitness_scores}\") \n",
    "\n",
    "    # track the best fitness score in the current generation. \n",
    "    # will be used later on test dataset as the best subset of features\n",
    "    current_best_fitness = np.max(fitness_scores)\n",
    "    current_best_individual = population[np.argmax(fitness_scores)]\n",
    "    \n",
    "    # count the number of selected features (those with values > 0.5)\n",
    "    num_selected_features = np.sum(current_best_individual > 0.5)\n",
    "    \n",
    "    ### PROGRESS UPDATE: print generation stats including number of selected features\n",
    "    print(f\"Best Fitness = {current_best_fitness:.7f}, Number of Features = {num_selected_features}, Selected Features = {current_best_individual}\")\n",
    "    \n",
    "    # update the overall best fitness and individual if necessary\n",
    "    best_fitness_overall = current_best_fitness\n",
    "    best_individual_overall = current_best_individual\n",
    "\n",
    "    # update the overall best fitness and individual if necessary\n",
    "    #if current_best_fitness > best_fitness_overall:\n",
    "    #    best_fitness_overall = current_best_fitness\n",
    "    #    best_individual_overall = current_best_individual\n",
    "    #    no_improvement_count = 0  # reset stagnation counter if there's improvement\n",
    "    #else:\n",
    "    #    no_improvement_count += 1  # increase stagnation counter if no improvement\n",
    "\n",
    "    ### PROGRESS UPDATE: print current state of best model\n",
    "    print(f\"Overall best fitness: {best_fitness_overall:.7f}, Overall best individual: {best_individual_overall}\") \n",
    "    \n",
    "    \n",
    "    ###___________________START___________________###\n",
    "    ### this 2 criteria were introduced to increase efficiency and terminate loop earlier.\n",
    "    ### At this stage I am not entirely sure it is neceassary nor they work properly.\n",
    "    ### will keep it commented out for now.\n",
    "\n",
    "    # check if the fitness threshold is reached\n",
    "    #if best_fitness_overall >= fitness_threshold:\n",
    "    #    termination_reason = f\"Desired fitness threshold of {fitness_threshold} reached.\"\n",
    "    #    break\n",
    "    \n",
    "    # check if there's been no improvement for stagnation_limit generations\n",
    "    #if no_improvement_count >= stagnation_limit:\n",
    "    #    termination_reason = f\"No improvement for {stagnation_limit} generations.\"\n",
    "    #    break\n",
    "    ###___________________END_____________________###\n",
    "\n",
    "\n",
    "    ###___________________START___________________###\n",
    "    # The purpose of this block of code to narrow down the individuals to those that have better fitness, \n",
    "    # creating a more fit population for the next generation. It mimics the principle of “survival of the fittest,” \n",
    "    # where better-performing individuals have a greater chance of reproducing.\n",
    "\n",
    "    # Selection: roulette wheel selection based on fitness (the better individual's fitness score the more probability to be selected)\n",
    "    probabilities = fitness_scores / np.sum(fitness_scores)\n",
    "    ### PROGRESS UPDATE: print probabilities\n",
    "    print(f\"Probabilities: {probabilities}\")\n",
    "\n",
    "    # Randomly selects individuals with replacement, using the probabilities array to bias the selection\n",
    "    selected_indices = np.random.choice(np.arange(population_size), size=population_size, p=probabilities)\n",
    "    ### PROGRESS UPDATE: print selected indicies\n",
    "    print(f\"Selected indicies: {selected_indices}\")\n",
    "\n",
    "    # creating a new array that contains only the chosen individuals from the current generation.\n",
    "    selected_population = population[selected_indices]\n",
    "    ### PROGRESS UPDATE: print new population\n",
    "    print(f\"Selected population: {selected_population}\")\n",
    "    ###___________________END_____________________###\n",
    "\n",
    "\n",
    "    \n",
    "    ###___________________START___________________###\n",
    "    # ELITISM:\n",
    "    # technique used to preserve the best solutions (individuals) from one generation to the next. \n",
    "    # It helps ensure that the best-found solutions so far are not lost during the evolution process.\n",
    "\n",
    "    elite_indices = np.argsort(fitness_scores)[-num_elites:]\n",
    "    ### PROGRESS UPDATE: print new population\n",
    "    print(f\"Elites indicies: {elite_indices}\")\n",
    "\n",
    "    elite_population = population[elite_indices]\n",
    "    ### PROGRESS UPDATE: print elite individuals\n",
    "    print(f\"Elites individuals: {elite_population}\")\n",
    "    ###___________________END_____________________###\n",
    "\n",
    "\n",
    "    ###___________________START___________________###\n",
    "    # Crossover: single-point crossover. The goal is to blend the genetic material of two parents \n",
    "    # in such a way that the offspring may inherit the best traits (features in this case) from each parent, \n",
    "    # promoting better solutions in future generations.\n",
    "\n",
    "    offspring = []\n",
    "    # the loop goes through pair of individuals iterating by 2\n",
    "    for i in range(0, population_size, 2):\n",
    "        # selecting 2 consequntive individuals as parents\n",
    "        parent1, parent2 = selected_population[i], selected_population[i+1]\n",
    "        ### PROGRESS UPDATE: parents \n",
    "        print(f\"Parent 1: {parent1}; Parent 2: {parent2}\")\n",
    "\n",
    "        # randomely defines where the “split” between the parents’ genes will occur\n",
    "        crossover_point = np.random.randint(1, n_features-1)\n",
    "        ### PROGRESS UPDATE: crossover point\n",
    "        print(f\"Crossover_point: {crossover_point}\")\n",
    "\n",
    "        child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
    "        child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
    "        ### PROGRESS UPDATE: children \n",
    "        print(f\"Child 1: {child1}; Child 2: {child2}\")\n",
    "        \n",
    "        offspring.extend([child1, child2])\n",
    "    \n",
    "    # The two newly created children are added to the offspring list using offspring.extend\n",
    "    # Ensure correct size by trimming excess if necessary\n",
    "    offspring = np.array(offspring[:population_size - num_elites])  # Ensure correct size by trimming excess if necessary\n",
    "    ###___________________END___________________###\n",
    "\n",
    "    \n",
    "\n",
    "    ###___________________START___________________### \n",
    "    # MUTATION:\n",
    "    # Flip features value (0 or 1) with a probability of mutation_rate and introduces diversity to the population \n",
    "    # by altering some genes in random locations, ensuring that the algorithm doesn’t rely only on existing solutions. \n",
    "    # This variation can help the algorithm escape local optima and explore a broader solution space, \n",
    "    # which is especially useful in complex optimization problems.\n",
    "    for individual in offspring:\n",
    "        # firstly create random values between 0 and 1 for each feature in individual, \n",
    "        # then compare to hyperparameter (mutation_rate) \n",
    "        # returning TRUE (less then hyperparameter value) or FALSE for each feature \n",
    "        mutation_mask = np.random.rand(n_features) < mutation_rate\n",
    "        # PROGRESS UPDATE: mutation_mask\n",
    "        print(f\"Mutation mask: {mutation_mask}\")\n",
    "\n",
    "        # if TRUE the feature will mutate (flip from 0 to 1 of vice versa)\n",
    "        individual[mutation_mask] = 1 - individual[mutation_mask]  # flip 0 to 1 or 1 to 0\n",
    "        ###___________________END___________________###\n",
    "\n",
    "\n",
    "    # Replace the population with offspring plus elite individuals\n",
    "    population = np.vstack((elite_population, offspring))\n",
    "\n",
    "### This statement relied on stagnation variable which is not in use, so commented out for now\n",
    "# if no termination reason set, the loop ran for the maximum number of generations\n",
    "#if termination_reason is None:\n",
    "#    termination_reason = f\"Reached maximum number of generations ({n_generations}).\"\n",
    "\n",
    "# print termination reason\n",
    "print(f\"Termination Reason: {termination_reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best Fitness Overall: {best_fitness_overall}; Number of Selected Features: {num_selected_features}; Best inidvidual Overall: {best_individual_overall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTION: Feature reduction with Genetics Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define overall function (just combined 4 code chunks above into 1 easy to use function)\n",
    "def ga_feature_reduction(X_train_sample, y_train_sample, X_test_sample, y_test_sample, \n",
    "                         population_size=20, n_generations=50, mutation_rate=0.1, \n",
    "                         num_elites=2, CLASSIFIER=\"rf\", metric=\"accuracy\"):\n",
    "    # Validate inputs\n",
    "    if population_size % 2 != 0:\n",
    "        raise ValueError(\"Population size must be an even number.\")\n",
    "    \n",
    "    # Get the number of features for individual length\n",
    "    n_features = X_train_sample.shape[1]\n",
    "    \n",
    "    # Initialize population with random binary values (0 or 1)\n",
    "    population = np.random.randint(2, size=(population_size, n_features))\n",
    "    \n",
    "    best_fitness_overall = 0\n",
    "    best_individual_overall = None\n",
    "    termination_reason = None\n",
    "\n",
    "    # Main loop for the genetic algorithm\n",
    "    for generation in range(n_generations):\n",
    "        print(f\"LOOP STEP: Generation {generation + 1}\")\n",
    "\n",
    "        # Calculate fitness scores for the current population\n",
    "        fitness_scores = np.array([fitness_function(individual, y_train_sample, y_test_sample, CLASSIFIER, metric) \n",
    "                                    for individual in population])\n",
    "        print(f\"Fitness Scores: {fitness_scores}\") \n",
    "        \n",
    "        # Track the best individual in the current generation\n",
    "        current_best_fitness = np.max(fitness_scores)\n",
    "        current_best_individual = population[np.argmax(fitness_scores)]\n",
    "        \n",
    "        # Update overall best fitness and individual if necessary\n",
    "        if current_best_fitness > best_fitness_overall:\n",
    "            best_fitness_overall = current_best_fitness\n",
    "            best_individual_overall = current_best_individual\n",
    "        \n",
    "        print(f\"Overall best fitness: {best_fitness_overall:.7f}, Overall best individual: {best_individual_overall}\")\n",
    "\n",
    "        # Selection: roulette wheel selection based on fitness\n",
    "        probabilities = fitness_scores / np.sum(fitness_scores)\n",
    "        selected_indices = np.random.choice(np.arange(population_size), size=population_size, p=probabilities)\n",
    "        selected_population = population[selected_indices]\n",
    "\n",
    "        # Elitism: preserve the best individuals\n",
    "        elite_indices = np.argsort(fitness_scores)[-num_elites:]\n",
    "        elite_population = population[elite_indices]\n",
    "\n",
    "        # Crossover: single-point crossover\n",
    "        offspring = []\n",
    "        for i in range(0, population_size, 2):\n",
    "            parent1, parent2 = selected_population[i], selected_population[i + 1]\n",
    "            crossover_point = np.random.randint(1, n_features - 1)\n",
    "            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
    "            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
    "            offspring.extend([child1, child2])\n",
    "\n",
    "        offspring = np.array(offspring[:population_size - num_elites])  # Ensure correct size by trimming excess\n",
    "\n",
    "        # Mutation: flip bits with a probability of mutation_rate\n",
    "        for individual in offspring:\n",
    "            mutation_mask = np.random.rand(n_features) < mutation_rate\n",
    "            individual[mutation_mask] = 1 - individual[mutation_mask]\n",
    "\n",
    "        # Replace the population with offspring plus elite individuals\n",
    "        population = np.vstack((elite_population, offspring))\n",
    "    \n",
    "    # Function output\n",
    "    print(f\"Termination Reason: {termination_reason}\")\n",
    "    print(f\"Best Fitness: {best_fitness_overall}\")\n",
    "    print(f\"Best Individual: {best_individual_overall}\")\n",
    "    return best_fitness_overall, best_individual_overall\n",
    "\n",
    "def fitness_function(individual, y_train_sample, y_test_sample, CLASSIFIER=\"rf\", metric=\"accuracy\"):\n",
    "    selected_features = np.where(individual == 1)[0]  # Select features based on individual's genes\n",
    "    \n",
    "    if len(selected_features) == 0:  # Avoid empty feature set\n",
    "        return 0\n",
    "    \n",
    "    X_train_selected = X_train_sample[:, selected_features]\n",
    "    X_test_selected = X_test_sample[:, selected_features]\n",
    "\n",
    "    if CLASSIFIER == 'lr':\n",
    "        lr = LogisticRegression()\n",
    "        lr.fit(X_train_selected, y_train_sample)\n",
    "        y_pred = lr.predict(X_test_selected)\n",
    "\n",
    "    elif CLASSIFIER == 'ada':\n",
    "        ada = AdaBoostClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=1984)\n",
    "        ada.fit(X_train_selected, y_train_sample)\n",
    "        y_pred = ada.predict(X_test_selected)\n",
    "\n",
    "    elif CLASSIFIER == 'rf':\n",
    "        rf = RandomForestClassifier()\n",
    "        rf.fit(X_train_selected, y_train_sample)\n",
    "        y_pred = rf.predict(X_test_selected)\n",
    "\n",
    "    # Calculate the chosen metric\n",
    "    if metric == 'accuracy':\n",
    "        score = accuracy_score(y_test_sample, y_pred)\n",
    "    elif metric == 'precision':\n",
    "        score = precision_score(y_test_sample, y_pred, average='weighted')\n",
    "    elif metric == 'recall':\n",
    "        score = recall_score(y_test_sample, y_pred, average='weighted')\n",
    "    elif metric == 'f1':\n",
    "        score = f1_score(y_test_sample, y_pred, average='weighted')\n",
    "    else:\n",
    "        raise ValueError(\"Invalid metric. Choose from 'accuracy', 'precision', 'recall', or 'f1'.\")\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_feature_reduction(\n",
    "    X_train_sample = X_train_sample,\n",
    "    y_train_sample = y_train_sample_2,\n",
    "    X_test_sample = X_test_sample,\n",
    "    y_test_sample = y_test_sample_2,\n",
    "    population_size = 6,\n",
    "    n_generations = 5,\n",
    "    mutation_rate = 0.1,\n",
    "    num_elites = 2,\n",
    "    CLASSIFIER = \"rf\",\n",
    "    metric = \"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection With GA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the GA function\n",
    "global_best_fitness, global_best_position = ga_feature_reduction(\n",
    "    X_train_sample = X_train_sample,\n",
    "    y_train_sample = y_train_sample_2,\n",
    "    X_test_sample = X_test_sample,\n",
    "    y_test_sample = y_test_sample_2,\n",
    "    population_size = 6,\n",
    "    n_generations = 3,\n",
    "    mutation_rate = 0.1,\n",
    "    num_elites = 2,\n",
    "    CLASSIFIER = \"lr\",\n",
    "    metric = \"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best features identified by GA\n",
    "selected_features = np.array(global_best_position, dtype=bool)\n",
    "# Apply the selected features to your datasets\n",
    "X_train_selected = X_train_sample[:, selected_features]\n",
    "X_test_selected = X_test_sample[:, selected_features]\n",
    "\n",
    "lr_benchmark.fit(X_train_selected, y_train_sample_2) \n",
    "y_pred_lr_ga_2 = lr_benchmark.predict(X_test_selected)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the classification report\n",
    "report_lr_ga_2 = classification_report(y_test_sample_2, y_pred_lr_ga_2, output_dict=True)\n",
    "print(classification_report(y_test_sample_2, y_pred_lr_ga_2))\n",
    "\n",
    "print(f\"Number of selected features: {X_train_selected.shape[1]}\")\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy_lr_ga_2 = accuracy_score(y_test_sample_2, y_pred_lr_ga_2)\n",
    "\n",
    "# print accuracy with 5 decimal places\n",
    "print(f\"Accuracy: {accuracy_lr_ga_2:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the confusion matrix\n",
    "cm_lr_ga_2 = confusion_matrix(y_test_sample_2, y_pred_lr_ga_2)\n",
    "\n",
    "# visualize the confusion matrix using seaborn\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(cm_lr_ga_2, annot=True, fmt='d', cmap='Blues', xticklabels=lr_benchmark.classes_, yticklabels=lr_benchmark.classes_)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the GA function\n",
    "global_best_fitness, global_best_position = ga_feature_reduction(\n",
    "    X_train_sample = X_train_sample,\n",
    "    y_train_sample = y_train_sample_6,\n",
    "    X_test_sample = X_test_sample,\n",
    "    y_test_sample = y_test_sample_6,\n",
    "    population_size = 6,\n",
    "    n_generations = 3,\n",
    "    mutation_rate = 0.1,\n",
    "    num_elites = 2,\n",
    "    CLASSIFIER = \"lr\",\n",
    "    metric = \"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best features identified by GA\n",
    "selected_features = np.array(global_best_position, dtype=bool)\n",
    "# Apply the selected features to your datasets\n",
    "X_train_selected = X_train_sample[:, selected_features]\n",
    "X_test_selected = X_test_sample[:, selected_features]\n",
    "\n",
    "lr_benchmark.fit(X_train_selected, y_train_sample_6) \n",
    "y_pred_lr_ga_6 = lr_benchmark.predict(X_test_selected)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the classification report\n",
    "report_lr_ga_6 = classification_report(y_test_sample_6, y_pred_lr_ga_6, output_dict=True)\n",
    "print(classification_report(y_test_sample_6, y_pred_lr_ga_6))\n",
    "\n",
    "print(f\"Number of selected features: {X_train_selected.shape[1]}\")\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy_lr_ga_6 = accuracy_score(y_test_sample_6, y_pred_lr_ga_6)\n",
    "\n",
    "# print accuracy with 5 decimal places\n",
    "print(f\"Accuracy: {accuracy_lr_ga_6:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the confusion matrix\n",
    "cm_lr_ga_6 = confusion_matrix(y_test_sample_6, y_pred_lr_ga_6)\n",
    "\n",
    "# visualize the confusion matrix using seaborn\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(cm_lr_ga_6, annot=True, fmt='d', cmap='Blues', xticklabels=lr_benchmark.classes_, yticklabels=lr_benchmark.classes_)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19 Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the GA function\n",
    "global_best_fitness, global_best_position = ga_feature_reduction(\n",
    "    X_train_sample = X_train_sample,\n",
    "    y_train_sample = y_train_sample_19,\n",
    "    X_test_sample = X_test_sample,\n",
    "    y_test_sample = y_test_sample_19,\n",
    "    population_size = 6,\n",
    "    n_generations = 3,\n",
    "    mutation_rate = 0.1,\n",
    "    num_elites = 2,\n",
    "    CLASSIFIER = \"lr\",\n",
    "    metric = \"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best features identified by GA\n",
    "selected_features = np.array(global_best_position, dtype=bool)\n",
    "# Apply the selected features to your datasets\n",
    "X_train_selected = X_train_sample[:, selected_features]\n",
    "X_test_selected = X_test_sample[:, selected_features]\n",
    "\n",
    "lr_benchmark.fit(X_train_selected, y_train_sample_19) \n",
    "y_pred_lr_ga_19 = lr_benchmark.predict(X_test_selected)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the classification report\n",
    "report_lr_ga_19 = classification_report(y_test_sample_19, y_pred_lr_ga_19, output_dict=True)\n",
    "print(classification_report(y_test_sample_19, y_pred_lr_ga_19))\n",
    "\n",
    "print(f\"Number of selected features: {X_train_selected.shape[1]}\")\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy_lr_ga_19 = accuracy_score(y_test_sample_19, y_pred_lr_ga_19)\n",
    "\n",
    "# print accuracy with 5 decimal places\n",
    "print(f\"Accuracy: {accuracy_lr_ga_19:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the confusion matrix\n",
    "cm_lr_ga_19 = confusion_matrix(y_test_sample_19, y_pred_lr_ga_19)\n",
    "\n",
    "# visualize the confusion matrix using seaborn\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(cm_lr_ga_19, annot=True, fmt='d', cmap='Blues', xticklabels=lr_benchmark.classes_, yticklabels=lr_benchmark.classes_)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
